{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkH6Q3gHFD1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q, K and K^T generation with fixed scale"
      ],
      "metadata": {
        "id": "ICkF31-YJNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Setup tokenizer\n",
        "# -----------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "text = '''The comprehensive analysis of transformer-based neural network architectures reveals\n",
        " that attention mechanisms fundamentally revolutionize how artificial intelligence systems process\n",
        " sequential data by enabling parallel computation while maintaining contextual relationships between\n",
        " distant elements in the input sequence, thereby significantly improving performance on natural language\n",
        " processing tasks such as machine translation, text summarization, question answering, sentiment analysis,\n",
        " and named entity recognition, which collectively demonstrate the remarkable versatility and effectiveness\n",
        " of these sophisticated deep learning models in understanding and generating human-like text across diverse\n",
        " linguistic patterns and semantic structures. The comprehensive analysis of transformer-based neural network architectures reveals\n",
        " that attention mechanisms fundamentally revolutionize how artificial intelligence systems process'''\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"Tokens: {tokens}\")\n",
        "print(f\"Token IDs: {ids}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Generate embeddings (demo)\n",
        "# -----------------------------\n",
        "# Simulate embedding lookup of dimension d_model\n",
        "d_model = 128\n",
        "np.random.seed(0)\n",
        "embeddings = np.random.randn(len(ids), d_model)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Define projection matrices for Q and K\n",
        "# -----------------------------\n",
        "d_k = 64  # query/key dimension\n",
        "W_Q = np.random.randn(d_model, d_k)\n",
        "W_K = np.random.randn(d_model, d_k)\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Compute Q, K, and Kᵀ\n",
        "# -----------------------------\n",
        "Q = embeddings @ W_Q\n",
        "K = embeddings @ W_K\n",
        "K_T = K.T  # <--- Transpose of K\n",
        "\n",
        "print(\"\\nFloat Q shape:\", Q.shape)\n",
        "print(\"Float K shape:\", K.shape)\n",
        "print(\"Float K^T shape:\", K_T.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Normalize and quantize to Q0.7 (INT8) with SAME SCALE\n",
        "# -----------------------------\n",
        "def quantize_Q07_same_scale(mat1, mat2):\n",
        "\n",
        "    # Find the maximum absolute value across BOTH matrices\n",
        "    max_abs_combined = max(np.max(np.abs(mat1)), np.max(np.abs(mat2)))\n",
        "\n",
        "    def quantize_with_scale(mat, scale):\n",
        "        # Normalize to [-1, 1] using the combined scale\n",
        "        norm = mat / scale\n",
        "        # Scale and round\n",
        "        q = np.round(norm * 128)\n",
        "        # Clip to [-128, 127]\n",
        "        q = np.clip(q, -128, 127)\n",
        "\n",
        "        return q.astype(np.int8)\n",
        "\n",
        "\n",
        "    q1 = quantize_with_scale(mat1, max_abs_combined)\n",
        "    q2 = quantize_with_scale(mat2, max_abs_combined)\n",
        "\n",
        "    return q1, q2, max_abs_combined\n",
        "\n",
        "# Use the same scale for both Q and K\n",
        "Q_int8, K_int8, shared_scale = quantize_Q07_same_scale(Q, K)\n",
        "\n",
        "K_T_int8 = K_int8.T  # <--- Quantized K transpose\n",
        "\n",
        "print(f\"\\nShared scale factor: {shared_scale}\")\n",
        "print(f\"Q range: [{np.min(Q_int8)}, {np.max(Q_int8)}]\")\n",
        "print(f\"K range: [{np.min(K_int8)}, {np.max(K_int8)}]\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Binary conversion (for hardware)\n",
        "# -----------------------------\n",
        "def int8_to_bin(vals):\n",
        "    \"\"\"Convert signed int8 array to 8-bit binary strings (2's complement).\"\"\"\n",
        "    flat_vals = vals.flatten()\n",
        "    bin_list = []\n",
        "    for v in flat_vals:\n",
        "        v_int = int(v)  # cast NumPy int8 to Python int\n",
        "        # ensure proper two’s complement representation\n",
        "        bin_val = format((v_int + (1 << 8)) % (1 << 8), '08b')\n",
        "        bin_list.append(bin_val)\n",
        "    return bin_list\n",
        "\n",
        "Q_bin = int8_to_bin(Q_int8)\n",
        "K_bin = int8_to_bin(K_int8)\n",
        "K_T_bin = int8_to_bin(K_T_int8)\n",
        "\n",
        "print(f\"\\nExample binary values (Q): {Q_bin[:8]}\")\n",
        "print(f\"Example binary values (K): {K_bin[:8]}\")\n",
        "print(f\"Example binary values (K^T): {K_T_bin[:8]}\")\n",
        "\n",
        "# Optionally save them to files for FPGA/RTL testbench\n",
        "# np.savetxt(\"Q_matrix.bin\", Q_bin, fmt=\"%s\")\n",
        "# np.savetxt(\"K_matrix.bin\", K_bin, fmt=\"%s\")\n",
        "# np.savetxt(\"K_T_matrix.bin\", K_T_bin, fmt=\"%s\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Save Binary Data to Text Files\n",
        "# -----------------------------\n",
        "\n",
        "def save_binary_matrix(bin_list, rows, cols, filename):\n",
        "    \"\"\"Save flattened binary list into text file with space-separated 8-bit values.\"\"\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        for i in range(rows):\n",
        "            row_data = bin_list[i * cols : (i + 1) * cols]\n",
        "            line = \" \".join(row_data)\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"\\nSaved: {filename}\")\n",
        "\n",
        "# Get matrix shapes\n",
        "rows_Q, cols_Q = Q_int8.shape\n",
        "rows_K, cols_K = K_int8.shape\n",
        "rows_KT, cols_KT = K_T_int8.shape\n",
        "\n",
        "# Save each as text file\n",
        "save_binary_matrix(Q_bin, rows_Q, cols_Q, \"Q_matrix_fixed_scale.txt\")\n",
        "save_binary_matrix(K_bin, rows_K, cols_K, \"K_matrix_fixed_scale.txt\")\n",
        "save_binary_matrix(K_T_bin, rows_KT, cols_KT, \"K_T_matrix_fixed_scale.txt\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR7z4q9tJKMp",
        "outputId": "d3323e59-290f-4159-ba52-3fefb9c2c444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['the', 'comprehensive', 'analysis', 'of', 'transform', '##er', '-', 'based', 'neural', 'network', 'architecture', '##s', 'reveals', 'that', 'attention', 'mechanisms', 'fundamentally', 'revolution', '##ize', 'how', 'artificial', 'intelligence', 'systems', 'process', 'sequential', 'data', 'by', 'enabling', 'parallel', 'computation', 'while', 'maintaining', 'context', '##ual', 'relationships', 'between', 'distant', 'elements', 'in', 'the', 'input', 'sequence', ',', 'thereby', 'significantly', 'improving', 'performance', 'on', 'natural', 'language', 'processing', 'tasks', 'such', 'as', 'machine', 'translation', ',', 'text', 'sum', '##mar', '##ization', ',', 'question', 'answering', ',', 'sentiment', 'analysis', ',', 'and', 'named', 'entity', 'recognition', ',', 'which', 'collectively', 'demonstrate', 'the', 'remarkable', 'versa', '##tility', 'and', 'effectiveness', 'of', 'these', 'sophisticated', 'deep', 'learning', 'models', 'in', 'understanding', 'and', 'generating', 'human', '-', 'like', 'text', 'across', 'diverse', 'linguistic', 'patterns', 'and', 'semantic', 'structures', '.', 'the', 'comprehensive', 'analysis', 'of', 'transform', '##er', '-', 'based', 'neural', 'network', 'architecture', '##s', 'reveals', 'that', 'attention', 'mechanisms', 'fundamentally', 'revolution', '##ize', 'how', 'artificial', 'intelligence', 'systems', 'process']\n",
            "Token IDs: [1996, 7721, 4106, 1997, 10938, 2121, 1011, 2241, 15756, 2897, 4294, 2015, 7657, 2008, 3086, 10595, 24670, 4329, 4697, 2129, 7976, 4454, 3001, 2832, 25582, 2951, 2011, 12067, 5903, 22334, 2096, 8498, 6123, 8787, 6550, 2090, 6802, 3787, 1999, 1996, 7953, 5537, 1010, 8558, 6022, 9229, 2836, 2006, 3019, 2653, 6364, 8518, 2107, 2004, 3698, 5449, 1010, 3793, 7680, 7849, 3989, 1010, 3160, 10739, 1010, 15792, 4106, 1010, 1998, 2315, 9178, 5038, 1010, 2029, 13643, 10580, 1996, 9487, 18601, 18724, 1998, 12353, 1997, 2122, 12138, 2784, 4083, 4275, 1999, 4824, 1998, 11717, 2529, 1011, 2066, 3793, 2408, 7578, 12158, 7060, 1998, 21641, 5090, 1012, 1996, 7721, 4106, 1997, 10938, 2121, 1011, 2241, 15756, 2897, 4294, 2015, 7657, 2008, 3086, 10595, 24670, 4329, 4697, 2129, 7976, 4454, 3001, 2832]\n",
            "\n",
            "Float Q shape: (128, 64)\n",
            "Float K shape: (128, 64)\n",
            "Float K^T shape: (64, 128)\n",
            "\n",
            "Shared scale factor: 45.25686655550364\n",
            "Q range: [-128, 127]\n",
            "K range: [-124, 120]\n",
            "\n",
            "Example binary values (Q): ['00000110', '00100000', '10110101', '11101000', '00110001', '11101100', '11110101', '00100100']\n",
            "Example binary values (K): ['00011011', '00010110', '00001100', '11110101', '00001001', '11101000', '00100111', '11110101']\n",
            "Example binary values (K^T): ['00011011', '00010001', '11101100', '00001001', '11100110', '00100000', '00010001', '00001001']\n",
            "\n",
            "Saved: Q_matrix_fixed_scale.txt\n",
            "\n",
            "Saved: K_matrix_fixed_scale.txt\n",
            "\n",
            "Saved: K_T_matrix_fixed_scale.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q, K and K^T in real**"
      ],
      "metadata": {
        "id": "2_sPd9v610Sr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Save Real (Floating-Point) Data to Text Files\n",
        "# -----------------------------\n",
        "\n",
        "def save_real_matrix(matrix, filename):\n",
        "    \"\"\"Save floating-point matrix to text file with space-separated values.\"\"\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        rows, cols = matrix.shape\n",
        "        for i in range(rows):\n",
        "            row_data = [f\"{matrix[i, j]:.8f}\" for j in range(cols)]\n",
        "            line = \" \".join(row_data)\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "\n",
        "Q_float_normalized = Q_int8.astype(float) / 128\n",
        "K_float_normalized = K_int8.astype(float) / 128\n",
        "K_T_float_normalized = K_T_int8.astype(float) / 128\n",
        "\n",
        "save_real_matrix(Q_float_normalized, \"Q_float_normalized_fixed_scale.txt\")\n",
        "save_real_matrix(K_float_normalized, \"K_float_normalized_fixed_scale.txt\")\n",
        "save_real_matrix(K_T_float_normalized, \"K_T_float_normalized_fixed_scale.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8vWk62t1w1S",
        "outputId": "de92e63c-c0a8-46b5-84ef-fee231f05fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: Q_float_normalized_fixed_scale.txt\n",
            "Saved: K_float_normalized_fixed_scale.txt\n",
            "Saved: K_T_float_normalized_fixed_scale.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Q and K^T in binary and decimal"
      ],
      "metadata": {
        "id": "qMWdtZAQPzzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Read matrices from text files\n",
        "# -----------------------------\n",
        "def read_binary_matrix_from_file(filename):\n",
        "    \"\"\"Read binary matrix from text file and convert to int8 array\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    matrix_data = []\n",
        "    for line in lines:\n",
        "        # Split each line by spaces to get individual 8-bit binary values\n",
        "        binary_values = line.strip().split()\n",
        "        # Convert each 8-bit binary string to signed int8\n",
        "        row_values = []\n",
        "        for bin_val in binary_values:\n",
        "            # Convert binary to int (handles 2's complement)\n",
        "            int_val = int(bin_val, 2)\n",
        "            # Convert to signed int8 (handle 2's complement for values > 127)\n",
        "            if int_val > 127:\n",
        "                int_val = int_val - 256\n",
        "            row_values.append(int_val)\n",
        "        matrix_data.append(row_values)\n",
        "\n",
        "    return np.array(matrix_data, dtype=np.int8)\n",
        "\n",
        "# Read Q and K^T matrices from files\n",
        "print(\"Reading matrices from files...\")\n",
        "Q_matrix = read_binary_matrix_from_file(\"Q_matrix_fixed_scale.txt\")\n",
        "KT_matrix = read_binary_matrix_from_file(\"K_T_matrix_fixed_scale.txt\")\n",
        "\n",
        "print(f\"Q matrix fixed scale shape: {Q_matrix.shape}\")\n",
        "print(f\"K^T matrix fixed scale shape: {KT_matrix.shape}\")\n",
        "print(f\"Q matrix fixed scale range: [{np.min(Q_matrix)}, {np.max(Q_matrix)}]\")\n",
        "print(f\"K^T matrix fixed scale range: [{np.min(KT_matrix)}, {np.max(KT_matrix)}]\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Compute matrix multiplication Q × K^T\n",
        "# -----------------------------\n",
        "print(\"\\nComputing Q × K^T...\")\n",
        "\n",
        "# Perform matrix multiplication (result will be in int32 to avoid overflow)\n",
        "attention_scores_int = Q_matrix.astype(np.int32) @ KT_matrix.astype(np.int32)\n",
        "print(f\"Attention scores shape: {attention_scores_int.shape}\")\n",
        "print(f\"Attention scores range: [{np.min(attention_scores_int)}, {np.max(attention_scores_int)}]\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Convert results to binary representation\n",
        "# -----------------------------\n",
        "def convert_to_Q07_and_binary(matrix, input_scale):\n",
        "    \"\"\"Convert int32 matrix to Q0.7 format and then to 8-bit binary\"\"\"\n",
        "\n",
        "    # First, convert back to floating point using the input scale\n",
        "    float_matrix = (matrix.astype(float) * input_scale * input_scale) / (128 * 128)\n",
        "\n",
        "    # Find the maximum absolute value for Q0.7 scaling\n",
        "    max_abs = np.max(np.abs(float_matrix))\n",
        "\n",
        "    # Normalize to [-1, 1] range\n",
        "    normalized = float_matrix / max_abs\n",
        "\n",
        "    # Convert to Q0.7 format (multiply by 128 and round)\n",
        "    q07_values = np.round(normalized * 128)\n",
        "\n",
        "    # Clip to valid Q0.7 range [-128, 127]\n",
        "    q07_values = np.clip(q07_values, -128, 127)\n",
        "\n",
        "    # Convert to int8\n",
        "    q07_int8 = q07_values.astype(np.int8)\n",
        "\n",
        "    # Convert to 8-bit binary\n",
        "    flat_vals = q07_int8.flatten()\n",
        "    bin_list = []\n",
        "    for v in flat_vals:\n",
        "        v_int = int(v)\n",
        "        # Handle 2's complement for 8-bit representation\n",
        "        bin_val = format((v_int + (1 << 8)) % (1 << 8), '08b')\n",
        "        bin_list.append(bin_val)\n",
        "\n",
        "    return q07_int8, bin_list, max_abs\n",
        "\n",
        "shared_scale = 31.85292984754848  # Replace with your actual shared_scale value\n",
        "\n",
        "attention_q07, attention_binary_q07, q07_scale = \\\n",
        "convert_to_Q07_and_binary(attention_scores_int, shared_scale)\n",
        "\n",
        "print(f\"\\nExample binary values: {attention_binary_q07[:5]}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Save results to text files\n",
        "# -----------------------------\n",
        "def save_matrix_decimal(matrix, filename):\n",
        "    \"\"\"Save matrix in decimal format\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        rows, cols = matrix.shape\n",
        "        for i in range(rows):\n",
        "            row_data = [str(matrix[i, j]) for j in range(cols)]\n",
        "            line = \" \".join(row_data)\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Saved decimal matrix: {filename}\")\n",
        "\n",
        "def save_matrix_binary(bin_list, rows, cols, filename):\n",
        "    \"\"\"Save matrix in binary format\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for i in range(rows):\n",
        "            row_data = bin_list[i * cols : (i + 1) * cols]\n",
        "            line = \" \".join(row_data)\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Saved binary matrix: {filename}\")\n",
        "\n",
        "# Get dimensions\n",
        "rows, cols = attention_scores_int.shape\n",
        "\n",
        "# Save in decimal format\n",
        "save_matrix_decimal(attention_q07, \"attention_scores_decimal.txt\")\n",
        "\n",
        "# Save in binary format\n",
        "save_matrix_binary(attention_binary_q07, rows, cols, \"attention_scores_binary.txt\")\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Display sample results\n",
        "# -----------------------------\n",
        "# print(f\"\\nSample attention scores (decimal):\")\n",
        "# print(attention_scores_int[:3, :3])  # Show first 3x3 submatrix\n",
        "\n",
        "# print(f\"\\nSample attention scores (binary, first 3 values):\")\n",
        "# for i in range(min(3, len(attention_binary))):\n",
        "#     decimal_val = attention_scores_int.flatten()[i]\n",
        "#     binary_val = attention_binary[i]\n",
        "#     print(f\"Decimal: {decimal_val:6d} | Binary: {binary_val}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Verification: Check file reading worked correctly\n",
        "# -----------------------------\n",
        "# print(f\"\\nVerification:\")\n",
        "# print(f\"Original Q matrix (first 2x2):\")\n",
        "# print(Q_matrix[:2, :2])\n",
        "# print(f\"Original K^T matrix (first 2x2):\")\n",
        "# print(KT_matrix[:2, :2])\n",
        "\n",
        "# Optional: If you have the shared_scale from previous code,\n",
        "# you can also compute the floating-point equivalent\n",
        "# shared_scale = your_scale_value  # from previous code\n",
        "# attention_scores_float = (attention_scores_int.astype(float) * shared_scale * shared_scale) / (128 * 128)\n",
        "# print(f\"Floating-point equivalent (scaled): {attention_scores_float[:2, :2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bmI37BWPzce",
        "outputId": "cfdc1cd8-d7f9-406d-ed00-fd4ca8aeb1e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading matrices from files...\n",
            "Q matrix fixed scale shape: (128, 64)\n",
            "K^T matrix fixed scale shape: (64, 128)\n",
            "Q matrix fixed scale range: [-128, 127]\n",
            "K^T matrix fixed scale range: [-124, 120]\n",
            "\n",
            "Computing Q × K^T...\n",
            "Attention scores shape: (128, 128)\n",
            "Attention scores range: [-34270, 29342]\n",
            "\n",
            "Example binary values: ['00000001', '11110100', '11110000', '00010000', '00101001']\n",
            "Saved decimal matrix: attention_scores_decimal.txt\n",
            "Saved binary matrix: attention_scores_binary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax computation - decimal, hex (unsigned Q0.12)"
      ],
      "metadata": {
        "id": "j0rl05ixqQ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Read your Q*K^T matrix from decimal file\n",
        "def read_decimal_matrix(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    matrix_data = []\n",
        "    for line in lines:\n",
        "        row_values = [float(val) for val in line.strip().split()]\n",
        "        matrix_data.append(row_values)\n",
        "\n",
        "    return np.array(matrix_data)\n",
        "\n",
        "# Compute softmax\n",
        "def compute_softmax(x):\n",
        "    x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Convert to Q0.12 unsigned\n",
        "def convert_to_q012(softmax_values):\n",
        "    q012_values = np.round(softmax_values * 4096)\n",
        "    return np.clip(q012_values, 0, 4095).astype(np.uint16)\n",
        "\n",
        "# Convert to hex\n",
        "def to_hex(values):\n",
        "    return [format(int(val), '03X') for val in values.flatten()]\n",
        "\n",
        "# Main computation\n",
        "qkt_matrix = read_decimal_matrix(\"attention_scores_decimal.txt\")\n",
        "softmax_decimal = compute_softmax(qkt_matrix)\n",
        "softmax_q012 = convert_to_q012(softmax_decimal)\n",
        "softmax_hex = to_hex(softmax_q012)\n",
        "\n",
        "rows, cols = softmax_decimal.shape\n",
        "\n",
        "# Save decimal softmax\n",
        "with open(\"softmax_q012_decimal_fixed_scale.txt\", 'w') as f:\n",
        "    for i in range(rows):\n",
        "        row_data = [f\"{softmax_decimal[i, j]:.8f}\" for j in range(cols)]\n",
        "        f.write(\" \".join(row_data) + \"\\n\")\n",
        "\n",
        "# Save Q0.12 hex\n",
        "with open(\"softmax_q012_hex_scale.txt\", 'w') as f:\n",
        "    for i in range(rows):\n",
        "        row_data = softmax_hex[i * cols : (i + 1) * cols]\n",
        "        f.write(\" \".join(row_data) + \"\\n\")"
      ],
      "metadata": {
        "id": "OLIbiRwAqY-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Iz6vdAXgomlG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSS7zit3fPh8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}