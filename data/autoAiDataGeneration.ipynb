{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Set The Flags**"
      ],
      "metadata": {
        "id": "-ImetGrVEamG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# Matrix Q and K are generated in binary and real format by default\n",
        "# To further generate Q and K^T product, Softmax matrcices\n",
        "# Kindly enable the flags below as required and run the cells\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Set flag to True to generate Q and K^T product matrix (real and binary)\n",
        "# ================================================================\n",
        "generate_q_kt_mat = False\n",
        "# ================================================================\n",
        "# Set flag to True to generate Q and K^T product matrix (real and binary)\n",
        "# ================================================================\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Set the flag to True to generate softmax (real and Hex)\n",
        "# ================================================================\n",
        "generate_softmax = False\n",
        "# ================================================================\n",
        "# Set the flag to True to generate softmax (real and Hex)\n",
        "# ================================================================\n"
      ],
      "metadata": {
        "id": "UkH6Q3gHFD1r"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q, K and K^T generation with fixed scale"
      ],
      "metadata": {
        "id": "ICkF31-YJNFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# 1. Load tokenizer + BERT model\n",
        "# ================================================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# Update the input text\n",
        "# ================================================================\n",
        "text = \"\"\"\n",
        "The comprehensive analysis of transformer-based neural network\n",
        "architectures reveals how attention mechanisms revolutionize\n",
        "artificial intelligence systems. The comprehensive analysis of transformer-based neural network\n",
        "architectures reveals how attention mechanisms revolutionize\n",
        "artificial intelligence systems. The analysis of transformer-based\n",
        "architectures reveals how attention mechanisms revolutionize\n",
        "AI systems.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# ================================================================\n",
        "# 2. Get input embeddings\n",
        "# ================================================================\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "hidden = outputs.hidden_states[0].squeeze(0)  # (seq_len, 768)\n",
        "hidden_np = hidden.cpu().numpy()\n",
        "\n",
        "seq_len, d_model = hidden_np.shape\n",
        "print(\"Hidden shape:\", hidden_np.shape)\n",
        "\n",
        "# ================================================================\n",
        "# 3. Extract REAL BERT W_Q and W_K from Layer 0\n",
        "# ================================================================\n",
        "layer = model.encoder.layer[0].attention.self\n",
        "\n",
        "W_Q = layer.query.weight.detach().cpu().numpy()   # (768, 768)\n",
        "W_K = layer.key.weight.detach().cpu().numpy()     # (768, 768)\n",
        "\n",
        "# ================================================================\n",
        "# 4. Compute REAL Q and K (full 768 dims)\n",
        "# ================================================================\n",
        "Q = hidden_np @ W_Q.T     # (seq_len, 768)\n",
        "K = hidden_np @ W_K.T     # (seq_len, 768)\n",
        "\n",
        "print(\"Full Q shape:\", Q.shape)\n",
        "print(\"Full K shape:\", K.shape)\n",
        "\n",
        "# ================================================================\n",
        "# 5. Reduce to ONE HEAD = 64 dims\n",
        "# ================================================================\n",
        "# Reshape into (seq_len, 12 heads, 64 dims per head)\n",
        "Q_heads = Q.reshape(seq_len, 12, 64)\n",
        "K_heads = K.reshape(seq_len, 12, 64)\n",
        "\n",
        "# Select head 0 (you may choose any 0–11)\n",
        "Q_64 = Q_heads[:, 0, :]   # (seq_len, 64)\n",
        "K_64 = K_heads[:, 0, :]   # (seq_len, 64)\n",
        "\n",
        "print(\"Reduced Q_64 shape:\", Q_64.shape)\n",
        "print(\"Reduced K_64 shape:\", K_64.shape)\n",
        "\n",
        "# ================================================================\n",
        "# 6. Shared-scale INT8 quantization (Q0.7)\n",
        "# ================================================================\n",
        "def quantize_Q07_same_scale(mat1, mat2):\n",
        "    max_abs = max(np.max(np.abs(mat1)), np.max(np.abs(mat2)))\n",
        "\n",
        "    def quantize(mat):\n",
        "        norm = mat / max_abs\n",
        "        q = np.round(norm * 128)\n",
        "        q = np.clip(q, -128, 127)\n",
        "        return q.astype(np.int8)\n",
        "\n",
        "    return quantize(mat1), quantize(mat2), max_abs\n",
        "\n",
        "Q_int8, K_int8, scale = quantize_Q07_same_scale(Q_64, K_64)\n",
        "\n",
        "print(\"\\nFixed Scale:\", scale)\n",
        "print(\"Q int8 range:\", Q_int8.min(), Q_int8.max())\n",
        "print(\"K int8 range:\", K_int8.min(), K_int8.max())\n",
        "\n",
        "# ================================================================\n",
        "# 7. Convert int8 → 8-bit binary strings\n",
        "# ================================================================\n",
        "def int8_to_bin(vals):\n",
        "    flat = vals.flatten()\n",
        "    bins = []\n",
        "    for v in flat:\n",
        "        b = format((int(v) + 256) % 256, \"08b\")  # 2's complement\n",
        "        bins.append(b)\n",
        "    return bins\n",
        "\n",
        "Q_bin = int8_to_bin(Q_int8)\n",
        "K_bin = int8_to_bin(K_int8)\n",
        "\n",
        "\n",
        "print(\"\\nExample Q binaries:\", Q_bin[:8])\n",
        "print(\"Example K binaries:\", K_bin[:8])\n",
        "\n",
        "# ================================================================\n",
        "# 8. Save binary matrix files (FPGA friendly)\n",
        "# ================================================================\n",
        "def save_binary_matrix(bin_list, rows, cols, filename):\n",
        "    with open(filename, \"w\") as f:\n",
        "        for r in range(rows):\n",
        "            row = bin_list[r * cols : (r + 1) * cols]\n",
        "            f.write(\" \".join(row) + \"\\n\")\n",
        "    print(\"Saved:\", filename)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Save Real (Floating-Point) Data to Text Files\n",
        "# -----------------------------\n",
        "\n",
        "def save_real_matrix(matrix, filename):\n",
        "    \"\"\"Save floating-point matrix to text file with space-separated values.\"\"\"\n",
        "    with open(filename, \"w\") as f:\n",
        "        rows, cols = matrix.shape\n",
        "        for i in range(rows):\n",
        "            row_data = [f\"{matrix[i, j]:.8f}\" for j in range(cols)]\n",
        "            line = \" \".join(row_data)\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Saved: {filename}\")\n",
        "\n",
        "rows_Q, cols_Q = Q_int8.shape  # (seq_len, 64)\n",
        "rows_K, cols_K = K_int8.shape  # (seq_len, 64)\n",
        "\n",
        "\n",
        "save_binary_matrix(Q_bin, rows_Q, cols_Q, \"Q_matrix_fixed_scale.txt\")\n",
        "save_binary_matrix(K_bin, rows_K, cols_K, \"K_matrix_fixed_scale.txt\")\n",
        "\n",
        "save_real_matrix(Q_int8.astype(float) / 128, \"Q_float_normalized_fixed_scale.txt\")\n",
        "save_real_matrix(K_int8.astype(float) / 128, \"K_float_normalized_fixed_scale.txt\")\n",
        "\n",
        "\n",
        "if (generate_q_kt_mat or generate_softmax):\n",
        "  # Transpose K for attention\n",
        "  K_T_int8 = K_int8.T\n",
        "\n",
        "  K_T_bin = int8_to_bin(K_T_int8)\n",
        "  rows_KT, cols_KT = K_T_int8.shape  # (64, seq_len)\n",
        "\n",
        "  save_binary_matrix(K_T_bin, rows_KT, cols_KT, \"K_T_matrix_fixed_scale.txt\")\n",
        "  save_real_matrix( K_T_int8.astype(float) / 128, \"K_T_float_normalized_fixed_scale.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AR7z4q9tJKMp",
        "outputId": "bb5c4759-899d-4727-f9c5-2cc3b422a7c7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden shape: (64, 768)\n",
            "Full Q shape: (64, 768)\n",
            "Full K shape: (64, 768)\n",
            "Reduced Q_64 shape: (64, 64)\n",
            "Reduced K_64 shape: (64, 64)\n",
            "\n",
            "Fixed Scale: 5.480356\n",
            "Q int8 range: -83 46\n",
            "K int8 range: -128 107\n",
            "\n",
            "Example Q binaries: ['00000011', '00000100', '00001001', '00000110', '11111010', '11110110', '00001011', '11110011']\n",
            "Example K binaries: ['00011110', '00000101', '00000110', '11110101', '11101000', '00010000', '11111101', '00001101']\n",
            "Saved: Q_matrix_fixed_scale.txt\n",
            "Saved: K_matrix_fixed_scale.txt\n",
            "Saved: Q_float_normalized_fixed_scale.txt\n",
            "Saved: K_float_normalized_fixed_scale.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Q and K^T in binary and real"
      ],
      "metadata": {
        "id": "qMWdtZAQPzzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Read matrices from text files\n",
        "# -----------------------------\n",
        "def read_binary_matrix_from_file(filename):\n",
        "    \"\"\"Read binary matrix from text file and convert to int8 array\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    matrix_data = []\n",
        "    for line in lines:\n",
        "        # Split each line by spaces to get individual 8-bit binary values\n",
        "        binary_values = line.strip().split()\n",
        "        # Convert each 8-bit binary string to signed int8\n",
        "        row_values = []\n",
        "        for bin_val in binary_values:\n",
        "            # Convert binary to int (handles 2's complement)\n",
        "            int_val = int(bin_val, 2)\n",
        "            # Convert to signed int8 (handle 2's complement for values > 127)\n",
        "            if int_val > 127:\n",
        "                int_val = int_val - 256\n",
        "            row_values.append(int_val)\n",
        "        matrix_data.append(row_values)\n",
        "\n",
        "    return np.array(matrix_data, dtype=np.int8)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Convert results to binary representation\n",
        "# -----------------------------\n",
        "def convert_to_Q07_and_binary(matrix, input_scale):\n",
        "    \"\"\"Convert int32 matrix to Q0.7 format and then to 8-bit binary\"\"\"\n",
        "\n",
        "    # First, convert back to floating point using the input scale\n",
        "    float_matrix = (matrix.astype(float) * input_scale * input_scale) / (128 * 128)\n",
        "\n",
        "    # Find the maximum absolute value for Q0.7 scaling\n",
        "    max_abs = np.max(np.abs(float_matrix))\n",
        "\n",
        "    # Normalize to [-1, 1] range\n",
        "    normalized = float_matrix / max_abs\n",
        "\n",
        "    # Convert to Q0.7 format (multiply by 128 and round)\n",
        "    q07_values = np.round(normalized * 128)\n",
        "\n",
        "    # Clip to valid Q0.7 range [-128, 127]\n",
        "    q07_values = np.clip(q07_values, -128, 127)\n",
        "\n",
        "    # Convert to int8\n",
        "    q07_int8 = q07_values.astype(np.int8)\n",
        "\n",
        "    # Convert to 8-bit binary\n",
        "    flat_vals = q07_int8.flatten()\n",
        "    bin_list = []\n",
        "    for v in flat_vals:\n",
        "        v_int = int(v)\n",
        "        # Handle 2's complement for 8-bit representation\n",
        "        bin_val = format((v_int + (1 << 8)) % (1 << 8), '08b')\n",
        "        bin_list.append(bin_val)\n",
        "\n",
        "    return q07_int8, bin_list, max_abs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Save results to text files\n",
        "# -----------------------------\n",
        "def save_matrix_decimal(matrix, filename):\n",
        "    \"\"\"Save matrix in decimal format\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        rows, cols = matrix.shape\n",
        "        for i in range(rows):\n",
        "            row_data = [str(matrix[i, j]) for j in range(cols)]\n",
        "            line = \" \".join(row_data)\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Saved decimal matrix: {filename}\")\n",
        "\n",
        "def save_matrix_binary(bin_list, rows, cols, filename):\n",
        "    \"\"\"Save matrix in binary format\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        for i in range(rows):\n",
        "            row_data = bin_list[i * cols : (i + 1) * cols]\n",
        "            line = \" \".join(row_data)\n",
        "            f.write(line + \"\\n\")\n",
        "    print(f\"Saved binary matrix: {filename}\")\n",
        "\n",
        "\n",
        "if (generate_q_kt_mat or generate_softmax):\n",
        "\n",
        "  # -----------------------------\n",
        "  # Compute matrix multiplication Q × K^T\n",
        "  # -----------------------------\n",
        "  print(\"\\nComputing Q × K^T...\")\n",
        "\n",
        "  # Read Q and K^T matrices from files\n",
        "  print(\"Reading matrices from files...\")\n",
        "  Q_matrix = read_binary_matrix_from_file(\"Q_matrix_fixed_scale.txt\")\n",
        "  KT_matrix = read_binary_matrix_from_file(\"K_T_matrix_fixed_scale.txt\")\n",
        "\n",
        "  # Perform matrix multiplication (result will be in int32 to avoid overflow)\n",
        "  attention_scores_int = Q_matrix.astype(np.int32) @ KT_matrix.astype(np.int32)\n",
        "  print(f\"Attention scores shape: {attention_scores_int.shape}\")\n",
        "  print(f\"Attention scores range: [{np.min(attention_scores_int)}, {np.max(attention_scores_int)}]\")\n",
        "\n",
        "  # print(f\"Q matrix fixed scale shape: {Q_matrix.shape}\")\n",
        "  print(f\"K^T matrix fixed scale shape: {KT_matrix.shape}\")\n",
        "  print(f\"Q matrix fixed scale range: [{np.min(Q_matrix)}, {np.max(Q_matrix)}]\")\n",
        "  print(f\"K^T matrix fixed scale range: [{np.min(KT_matrix)}, {np.max(KT_matrix)}]\")\n",
        "\n",
        "  attention_q07, attention_binary_q07, q07_scale = \\\n",
        "  convert_to_Q07_and_binary(attention_scores_int, scale)\n",
        "\n",
        "  print(f\"\\nExample binary values: {attention_binary_q07[:5]}\")\n",
        "\n",
        "  # Get dimensions\n",
        "  rows, cols = attention_scores_int.shape\n",
        "\n",
        "  # Save in decimal format\n",
        "  save_matrix_decimal(attention_q07, \"attention_scores_decimal.txt\")\n",
        "\n",
        "  # Save in binary format\n",
        "  save_matrix_binary(attention_binary_q07, rows, cols, \"attention_scores_binary.txt\")\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"Q K^T product matrix is not generated as generate_q_kt_mat flag is disabled.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9bmI37BWPzce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a83a84-294c-4e1e-892e-dd76938bff79"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q K^T product matrix is not generated as generate_q_kt_mat flag is disabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax computation - real, hex (unsigned Q0.12)"
      ],
      "metadata": {
        "id": "j0rl05ixqQ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Read your Q*K^T matrix from decimal file\n",
        "def read_decimal_matrix(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    matrix_data = []\n",
        "    for line in lines:\n",
        "        row_values = [float(val) for val in line.strip().split()]\n",
        "        matrix_data.append(row_values)\n",
        "\n",
        "    return np.array(matrix_data)\n",
        "\n",
        "# Compute softmax\n",
        "def compute_softmax(x):\n",
        "    x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
        "    exp_x = np.exp(x_shifted)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Convert to Q0.12 unsigned\n",
        "def convert_to_q012(softmax_values):\n",
        "    q012_values = np.round(softmax_values * 4096)\n",
        "    return np.clip(q012_values, 0, 4095).astype(np.uint16)\n",
        "\n",
        "# Convert to hex\n",
        "def to_hex(values):\n",
        "    return [format(int(val), '03X') for val in values.flatten()]\n",
        "\n",
        "\n",
        "if (generate_softmax):\n",
        "\n",
        "  # Main computation\n",
        "  qkt_matrix = read_decimal_matrix(\"attention_scores_decimal.txt\")\n",
        "  softmax_decimal = compute_softmax(qkt_matrix)\n",
        "  softmax_q012 = convert_to_q012(softmax_decimal)\n",
        "  softmax_hex = to_hex(softmax_q012)\n",
        "\n",
        "  rows, cols = softmax_decimal.shape\n",
        "\n",
        "  # Save decimal softmax\n",
        "  with open(\"softmax_q012_decimal_fixed_scale.txt\", 'w') as f:\n",
        "      for i in range(rows):\n",
        "          row_data = [f\"{softmax_decimal[i, j]:.8f}\" for j in range(cols)]\n",
        "          f.write(\" \".join(row_data) + \"\\n\")\n",
        "      print(\"Saved file softmax_q012_decimal_scale.txt\")\n",
        "\n",
        "  # Save Q0.12 hex\n",
        "  with open(\"softmax_q012_hex_scale.txt\", 'w') as f:\n",
        "      for i in range(rows):\n",
        "          row_data = softmax_hex[i * cols : (i + 1) * cols]\n",
        "          f.write(\" \".join(row_data) + \"\\n\")\n",
        "      print(\"Saved file softmax_q012_hex_scale.txt\")\n",
        "\n",
        "else:\n",
        "  print(\"Softmax won\\'t be generated as generate_softmax flag is disabled.\")\n"
      ],
      "metadata": {
        "id": "OLIbiRwAqY-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c6ed8f9-9bdf-433b-dff0-f7b00c7369c9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax won't be generated as generate_softmax flag is disabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Iz6vdAXgomlG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iSS7zit3fPh8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}